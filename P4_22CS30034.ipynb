{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Karan Kumar Sethi**\n",
        "**Roll No:- 22CS30034**\n"
      ],
      "metadata": {
        "id": "YiNMRHVpt7ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary modules**"
      ],
      "metadata": {
        "id": "tbdHKu3uuNav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMP3AkFwhiTd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Class Implementation**"
      ],
      "metadata": {
        "id": "XF7taxqRuYQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
        "        # Initialize neural network parameters\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Randomly initialize weights (He initialization)\n",
        "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2.0 / self.input_size)\n",
        "        self.b1 = np.zeros((1, self.hidden_size))  # Bias for hidden layer\n",
        "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2.0 / self.hidden_size)\n",
        "        self.b2 = np.zeros((1, self.output_size))  # Bias for output layer\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\" Sigmoid activation function \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        \"\"\" Derivative of sigmoid function (used in backpropagation) \"\"\"\n",
        "        return z * (1 - z)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\" Forward propagation to calculate predicted output \"\"\"\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)  # Hidden layer activations\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        output = self.sigmoid(self.z2)  # Output layer activations\n",
        "        return output\n",
        "\n",
        "    def backprop(self, X, y, output):\n",
        "        \"\"\" Backpropagation to compute gradients and update weights \"\"\"\n",
        "        # Calculate error in output layer\n",
        "        output_error = y - output  # Error in output\n",
        "        output_delta = output_error * self.sigmoid_derivative(output)  # Gradient of loss with respect to output\n",
        "\n",
        "        # Calculate error in hidden layer\n",
        "        hidden_error = np.dot(output_delta, self.W2.T)  # Error propagated to hidden layer\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.a1)  # Gradient of hidden layer activations\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W2 += self.learning_rate * np.dot(self.a1.T, output_delta)  # Update W2\n",
        "        self.b2 += self.learning_rate * np.sum(output_delta, axis=0, keepdims=True)  # Update bias for output layer\n",
        "        self.W1 += self.learning_rate * np.dot(X.T, hidden_delta)  # Update W1\n",
        "        self.b1 += self.learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)  # Update bias for hidden layer\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        \"\"\" Train the model for a fixed number of epochs \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            self.backprop(X, y, output)\n",
        "\n",
        "    def calculate_loss(self, y, output):\n",
        "        \"\"\" Calculate the Mean Squared Error (MSE) loss \"\"\"\n",
        "        return np.mean((y - output) ** 2)\n"
      ],
      "metadata": {
        "id": "qDgHACDXuc_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing and Normalization**"
      ],
      "metadata": {
        "id": "ItBPEWBxuial"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data():\n",
        "    \"\"\" Load and normalize the Boston Housing dataset \"\"\"\n",
        "    df = pd.read_csv('housing.csv')\n",
        "    X_data = df.drop(columns=['MEDV']).to_numpy()\n",
        "    y_data = df['MEDV'].to_numpy().reshape(-1, 1)\n",
        "\n",
        "    # Normalize data using min-max scaling (optional, adjust as needed)\n",
        "    X_data = (X_data - X_data.min(axis=0)) / (X_data.max(axis=0) - X_data.min(axis=0))\n",
        "    y_data = (y_data - y_data.min()) / (y_data.max() - y_data.min())\n",
        "\n",
        "    return X_data, y_data"
      ],
      "metadata": {
        "id": "dKXtS06FupnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-validation function**"
      ],
      "metadata": {
        "id": "LEeVfMqCuwA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_k_fold_cross_validation(X, y, hidden_size, learning_rate, epochs, k_folds=5):\n",
        "    \"\"\" Perform k-fold cross-validation on the dataset \"\"\"\n",
        "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=1)\n",
        "    losses = []\n",
        "\n",
        "    for train_index, test_index in kfold.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # Initialize and train neural network\n",
        "        nn = NeuralNetwork(input_size=X.shape[1], hidden_size=hidden_size, output_size=1, learning_rate=learning_rate)\n",
        "        nn.train(X_train, y_train, epochs)\n",
        "\n",
        "        # Calculate loss on test set\n",
        "        output = nn.forward(X_test)\n",
        "        loss = nn.calculate_loss(y_test, output)\n",
        "        losses.append(loss)\n",
        "\n",
        "    return np.mean(losses)"
      ],
      "metadata": {
        "id": "cU7xOwU9u0VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main function to run experiments with user inputs**"
      ],
      "metadata": {
        "id": "7qqcLwsau66p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments():\n",
        "    \"\"\" Main experiment loop for different configurations of the neural network \"\"\"\n",
        "    # Load and preprocess dataset\n",
        "    X_data, y_data = load_and_preprocess_data()\n",
        "\n",
        "    # Configuration settings from user input\n",
        "    hidden_size = int(input(\"Enter number of neurons in the hidden layer: \"))\n",
        "    learning_rate = float(input(\"Enter the learning rate: \"))\n",
        "    epochs = 1000\n",
        "\n",
        "    print(\"5-fold cross-validation results:\")\n",
        "    avg_loss_5fold = perform_k_fold_cross_validation(X_data, y_data, hidden_size, learning_rate, epochs, k_folds=5)\n",
        "    print(f'Configuration: {hidden_size} hidden neurons, learning rate: {learning_rate}, Average Loss (5-Fold CV): {avg_loss_5fold}')\n",
        "\n",
        "    print(\"10-fold cross-validation results:\")\n",
        "    avg_loss_10fold = perform_k_fold_cross_validation(X_data, y_data, hidden_size, learning_rate, epochs, k_folds=10)\n",
        "    print(f'Configuration: {hidden_size} hidden neurons, learning rate: {learning_rate}, Average Loss (10-Fold CV): {avg_loss_10fold}')"
      ],
      "metadata": {
        "id": "c81ZQRwcvBEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the experiments**"
      ],
      "metadata": {
        "id": "pxBZZBWKvHAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runExpr = input(\"Do you want to run the Experiment (y/n): \")\n",
        "while(runExpr.lower() == 'y'):\n",
        "    run_experiments()\n",
        "    runExpr = input(\"Do you want to run Experiment again (y/n): \")\n"
      ],
      "metadata": {
        "id": "m9zlYP8xvNTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}